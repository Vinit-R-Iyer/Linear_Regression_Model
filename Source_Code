--------------------
IMPORTING LIBRARIES
--------------------
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sea
import pandas as pd

------------------
IMPORTING DATASET
------------------
data = pd.read_csv("Raw_Housing_Prices.csv")
data.head()
data.info()

--------------------------
EXPLORING TARGET VARIABLE
--------------------------
data["Sale Price"].describe()
data["Sale Price"].plot.box()
data["Sale Price"].plot.hist()

q1 = data["Sale Price"].quantile(0.25)
q3 = data["Sale Price"].quantile(0.75)
q1, q3
iqr = q3-q1
iqr
upper_limit = q3 + 1.5*iqr
lower_limit = q1 - 1.5*iqr
upper_limit, lower_limit

def new_limit(value):
if value > upper_limit:
return upper_limit
if value < lower_limit:
return lower_limit
else:
return value
data["Sale Price"] = data["Sale Price"].apply(new_limit)
data["Sale Price"].describe()

data["Sale Price"].plot.box()
data["Sale Price"].plot.hist()
data.isnull().sum()

data["Sale Price"].dropna(inplace = True)
data["Sale Price"].isnull().sum()
data.info()

--------------------------
EXPLORING OTHER VARIABLES
--------------------------
#Float variables
numerical_columns = ["No of Bathrooms", "Flat Area (in Sqft)", "Lot Area (in Sqft)", "Area of the House from Basement (in Sqft)", "Latitude", "Longitude", "Living Area after Renovation (in Sqft)"]
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values = np.nan, strategy = "median")
data[numerical_columns] = imputer.fit_transform(data[numerical_columns])
data.info()

---------------------------------
TRANSFORMING VARIABLES - ZIPCODE
---------------------------------
imputer = SimpleImputer(missing_values = np.nan, strategy = "most_frequent")
data["Zipcode"] = imputer.fit_transform(data["Zipcode"].values.reshape(-1,1))
data["Zipcode"].shape
column = data["Zipcode"].values.reshape(-1,1)
column.shape
imputer = SimpleImputer(missing_values = np.nan, strategy = "most_frequent")
data["Zipcode"] = imputer.fit_transform(column)
data.info()

-----------------------------
TRANSFORMING OTHER VARIABLES
-----------------------------
data["No of Times Visited"].unique()
mapping = {"None" : "0", "Once" : "1", "Twice" : "2", "Thrice" : "3", "Four" : "4"}
data["No of Times Visited"] = data["No of Times Visited"].map(mapping)
data["No of Times Visited"].unique()

-------------------------------
CREATING MORE VALUABLE COLUMNS
-------------------------------
data["Ever Renovated"] = np.where(data["Renovated Year"] == 0, "No", "Yes")
data.head(2)
data['Purchase Year'] = pd.DatetimeIndex(data['Date House was Sold']).year
data["Year since Renovation"] = np.where(data["Ever Renovated"] == "Yes", abs(data["Purchase Year"] - data["Renovated Year"]), 0)
data.head(2)
data.drop(columns = ["Purchase Year", "Date House was Sold", "Renovated Year"], inplace = True)
data.head(2)

-----------------------
GROUPING THE VARIABLES
-----------------------
data.drop(columns = "ID", inplace = True)
data.head(2)
data["Condition of the House"].value_counts()
data.groupby("Condition of the House")["Sale Price"].mean().sort_values().plot(kind = "bar")
data.groupby("Waterfront View")["Sale Price"].mean().sort_values().plot(kind = "bar")
data.groupby("Ever Renovated")["Sale Price"].mean().sort_values().plot(kind = "bar")
data.groupby("Zipcode")["Sale Price"].mean().sort_values().plot(kind = "bar")

------------------------------------------------------------
SPLITTING THE DATASET INTO INDEPENDENT AND TARGET VARIABLES
------------------------------------------------------------
data.dropna(inplace = True)
X = data.drop(columns = ["Sale Price"])
Y = data["Sale Price"]

----------------------------
TRANSFORMATION OF VARIABLES
----------------------------
def distribution(data ,var):
plt.figure(figsize = (len(var)*10,10), dpi = 200)
for j,i in enumerate(var):
plt.subplot(1,len(var),j+1)
plt.hist(data[i])
plt.title(i)
numerical_columns = ['No of Bedrooms', 'No of Bathrooms', 'Lot Area (in Sqft)', 'No of Floors', 'Area of the House from Basement (in Sqft)', 'Basement Area (in Sqft)', 'Age of House (in Years)', 'Latitude', 'Longitude', 'Living Area after Renovation (in Sqft)', 'Lot Area after Renovation (in Sqft)', 'Year since Renovation']
for i in numerical_columns:
X[i] = pd.to_numeric(X[i])
distribution(X, numerical_columns)

def right_skew(x):
return np.log(abs(x+500))
right_skew_variables = ['No of Bedrooms', 'No of Bathrooms', 'Lot Area (in Sqft)', 'No of Floors', 'Area of the House from Basement (in Sqft)', 'Basement Area (in Sqft)', 'Longitude', 'Living Area after Renovation (in Sqft)', 'Lot Area after Renovation (in Sqft)', 'Year since Renovation']
for i in right_skew_variables:
X[i] = X[i].map(right_skew)
# removing infinite values
X = X.replace(np.inf, np.nan)
X.dropna(inplace=True)
distribution(X, numerical_columns)

-----------------------
SCALING OF THE DATASET
-----------------------
X.head()
Y.head()
X["Waterfront View"] = X["Waterfront View"].map({"No" : "0", "Yes" : "1"})
X["Condition of the House"] = X["Condition of the House"].map({"Bad" : "1", "Okay" : "2", "Fair" : "3", "Good" : "4", "Excellent" : "5"})
X["Ever Renovated"] = X["Ever Renovated"].map({"No" : "0", "Yes" : "1"})
X.head()
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
Y = data["Sale Price"]
X1 = scaler.fit_transform(X)
X = pd.DataFrame(data = X1, columns = X.columns)
X.head()

------------------------------------
MULTICOLLINEARITY CHECK AND REMOVAL
------------------------------------
X.corr()

--------------------------------------------
CALCULATING VARAIBLE INFLATION FACTOR (VIF)
--------------------------------------------
from statsmodels.stats.outliers_influence import variance_inflation_factor
vif_data = X[:]
VIF = pd.Series([variance_inflation_factor(vif_data.values, i) for i in range(vif_data.shape[1])], index = vif_data.columns)
VIF

def Multicollinearity_remover(data):
vif = pd.Series([variance_inflation_factor(data.values, i) for i in range(data.shape[1])], index = data.columns)
if vif.max() > 5:
print(vif[vif == vif.max()].index[0],'has been removed')
data = data.drop(columns = [vif[vif == vif.max()].index[0]])
return data
else:
print('No Multicollinearity present anymore')
return data
for i in range(10):
vif_data = Multicollinearity_remover(vif_data)
vif_data.head()

VIF = pd.Series([variance_inflation_factor(vif_data.values, i) for i in range(vif_data.shape[1])], index = vif_data.columns)
VIF, len(vif_data.columns)

X = vif_data[:]
Y = data["Sale Price"]

-----------------------------------
CREATING TRAINING AND TEST DATASET
-----------------------------------
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 101)
x_train.shape, x_test.shape, y_train.shape, y_test.shape

-------------------
TRAINING THE MODEL
-------------------
from sklearn.linear_model import LinearRegression
lr = LinearRegression(normalize = True)
lr.fit(x_train, y_train)

lr.coef_

predictions = lr.predict(x_test)
lr.score(x_test, y_test)

-------------------
PLOTTING RESIDUALS
-------------------
residuals = predictions - y_test
residual_table = pd.DataFrame({'residuals':residuals,
'predictions':predictions})
residual_table = residual_table.sort_values( by = 'predictions')
z = [i for i in range(int(residual_table['predictions'].max()))]
k = [0 for i in range(int(residual_table['predictions'].max()))]
plt.figure(dpi = 130, figsize = (17,7))
plt.scatter( residual_table['predictions'], residual_table['residuals'], color = 'red', s = 2)
plt.plot(z, k, color = 'green', linewidth = 3, label = 'regression line')
plt.ylim(-800000, 800000)
plt.xlabel('fitted points (ordered by predictions)')
plt.ylabel('residuals')
plt.title('residual plot')
plt.legend()
plt.show()

----------------------------
PLOTTING ERROR DISTRIBUTION
----------------------------
plt.figure(dpi = 100, figsize = (10,7))
plt.hist(residual_table['residuals'], color = 'red', bins = 200)
plt.xlabel('residuals')
plt.ylabel('frequency')
plt.title('distribution of residuals')
plt.show()

-----------------------
MODELLING COEFFICIENTS
-----------------------
coefficients_table = pd.DataFrame({'column': x_train.columns,
'coefficients': lr.coef_})
coefficient_table = coefficients_table.sort_values(by = 'coefficients')
plt.figure(figsize=(8, 6), dpi=120)
x = coefficient_table['column']
y = coefficient_table['coefficients']
plt.barh( x, y)
plt.xlabel( "Coefficients")
plt.ylabel('Variables')
plt.title('Normalized Coefficient plot')
plt.show()
